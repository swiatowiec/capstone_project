{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os, re\n",
    "import configparser\n",
    "import pandas as pd\n",
    "from datetime import timedelta, datetime\n",
    "from pyspark.sql import SparkSession, SQLContext, GroupedData\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, DoubleType\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import date_add as d_add\n",
    "from pyspark.sql.types import DoubleType\n",
    "INPUT_FORMAT = 'csv'\n",
    "\n",
    "def create_spark_session():\n",
    "    \"\"\"\n",
    "    This function creates a session with Spark, the entry point to programming Spark with the Dataset and DataFrame API.\n",
    "    \"\"\"\n",
    "    os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "    os.environ[\"PATH\"] = \"/opt/conda/bin:/opt/spark-2.4.3-bin-hadoop2.7/bin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/lib/jvm/java-8-openjdk-amd64/bin\"\n",
    "    os.environ[\"SPARK_HOME\"] = \"/opt/spark-2.4.3-bin-hadoop2.7\"\n",
    "    os.environ[\"HADOOP_HOME\"] = \"/opt/spark-2.4.3-bin-hadoop2.7\"\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    return spark\n",
    "\n",
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- port_of_admission: string (nullable = true)\n",
      " |-- arrival_state: string (nullable = true)\n",
      " |-- birth_city: string (nullable = true)\n",
      " |-- birth_year: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      " |-- Mode: string (nullable = true)\n",
      " |-- visa: string (nullable = true)\n",
      " |-- arrival_date: date (nullable = true)\n",
      " |-- departure_date: date (nullable = true)\n",
      " |-- native_country: string (nullable = true)\n",
      " |-- residence_country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Load immigration data and rename default columns\n",
    "immigration = spark.read.load('immigration_data_sample.csv', format=INPUT_FORMAT, header=True).select('*')\\\n",
    "    .withColumnRenamed('cicid','ID')\\\n",
    "    .withColumnRenamed('i94yr','year')\\\n",
    "    .withColumnRenamed('i94mon','month')\\\n",
    "    .withColumnRenamed('i94port','port_of_admission')\\\n",
    "    .withColumnRenamed('i94mode','mode_of_trans')\\\n",
    "    .withColumnRenamed('i94addr','arrival_state')\\\n",
    "    .withColumnRenamed('i94visa','visa_code')\\\n",
    "    .withColumnRenamed('biryear','birth_year')\\\n",
    "    .withColumnRenamed('i94bir','birth_city')\n",
    " \n",
    "#Drop not useful columns\n",
    "not_useful = [\"count\", \"entdepa\", \"entdepd\", \"matflag\", \"fltno\", \"dtaddto\", \"admnum\", \"i94bir\", \"dtadfile\", \"visapost\", \"occup\", \"entdepu\", \"insnum\"]\n",
    "immigration = immigration.drop(*not_useful)\n",
    "\n",
    "# drop null records from i94addr column\n",
    "immigration = immigration.na.drop(subset=[\"arrival_state\"])\n",
    "#Change I94MODE code to string\n",
    "transport_lookup = spark.read.load(\"lookup/I94MODE.csv\", format=\"csv\", columns=\"*\", header=True).select('*').withColumnRenamed('ID', 'ID_TMP')\n",
    "immigration = immigration.withColumn(\"mode_of_trans\",col(\"mode_of_trans\").cast(\"int\"))\n",
    "immigration = immigration.join(transport_lookup, transport_lookup[\"ID_TMP\"] == immigration['mode_of_trans'], \"left\")\n",
    "immigration = immigration.drop('ID_TMP', 'mode_of_trans')\n",
    "\n",
    "#Cast to int\n",
    "immigration = immigration.withColumn(\"year\",col(\"year\").cast(\"int\"))\\\n",
    "    .withColumn(\"month\",col(\"month\").cast(\"int\"))\\\n",
    "    .withColumn(\"ID\",col(\"ID\").cast(\"int\"))\\\n",
    "    .withColumn(\"birth_year\",col(\"birth_year\").cast(\"int\"))\n",
    "\n",
    "#Decode I94VISA\n",
    "visa_lookup = spark.read.load(\"lookup/I94VISA.csv\", format=\"csv\", columns=\"*\", header=True).select('*').withColumnRenamed('ID', 'ID_TMP')\\\n",
    ".withColumnRenamed('Type', 'visa')\n",
    "immigration = immigration.withColumn(\"visa_code\",col(\"visa_code\").cast(\"int\"))\n",
    "immigration = immigration.join(visa_lookup, visa_lookup[\"ID_TMP\"] == immigration['visa_code'], \"left\")\n",
    "immigration = immigration.drop('ID_TMP', 'visa_code')\n",
    "\n",
    "#Convert SAS datatype to dataframe\n",
    "immigration = immigration.withColumn(\"base_sas_date\", to_date(lit(\"01/01/1960\"), \"MM/dd/yyyy\")) \\\n",
    "            .withColumn(\"arrival_date\", expr(\"date_add(base_sas_date, arrdate)\")) \\\n",
    "            .withColumn(\"departure_date\", expr(\"date_add(base_sas_date, depdate)\")) \\\n",
    "            .drop(\"base_sas_date\", \"arrdate\", \"depdate\")\n",
    "\n",
    "#Decode I94CITY\n",
    "city_lookup = spark.read.load(\"lookup/I94CIT_I94RES.csv\", format=\"csv\", columns=\"*\", header=True).select('*')\n",
    "immigration = immigration.withColumn(\"i94cit\",col(\"i94cit\").cast(\"int\"))\\\n",
    "    .withColumn(\"i94res\",col(\"i94res\").cast(\"int\"))\n",
    "\n",
    "immigration = immigration.join(city_lookup, city_lookup[\"Code\"] == immigration['i94cit'], \"left\")\\\n",
    "    .withColumnRenamed('I94CTRY', 'native_country')\\\n",
    "    .drop('i94cit', 'Code')\n",
    "\n",
    "immigration = immigration.join(city_lookup, city_lookup[\"Code\"] == immigration['i94res'], \"left\")\\\n",
    "    .withColumnRenamed('I94CTRY', 'residence_country')\\\n",
    "    .drop('i94res', 'Code')\n",
    "\n",
    "immigration.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median_age: string (nullable = true)\n",
      " |-- Male_population: integer (nullable = true)\n",
      " |-- Female_population: integer (nullable = true)\n",
      " |-- Total_population: integer (nullable = true)\n",
      " |-- Number_of_veterans: integer (nullable = true)\n",
      " |-- Foreign-born: integer (nullable = true)\n",
      " |-- Average_household_size: string (nullable = true)\n",
      " |-- State_code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demographics = spark.read.load('us-cities-demographics.csv', format=INPUT_FORMAT, sep=';', header=True).select('*')\\\n",
    "    .withColumnRenamed('Median Age', 'Median_age')\\\n",
    "    .withColumnRenamed('Male Population', 'Male_population')\\\n",
    "    .withColumnRenamed('Female Population', 'Female_population')\\\n",
    "    .withColumnRenamed('Total Population', 'Total_population')\\\n",
    "    .withColumnRenamed('Number of Veterans', 'Number_of_veterans')\\\n",
    "    .withColumnRenamed('Average Household Size', 'Average_household_size')\\\n",
    "    .withColumnRenamed('State Code', 'State_code')\\\n",
    "    .withColumn(\"Male_population\",col(\"Male_population\").cast(\"int\"))\\\n",
    "    .withColumn(\"Female_population\",col(\"Female_population\").cast(\"int\"))\\\n",
    "    .withColumn(\"Total_population\",col(\"Total_population\").cast(\"int\"))\\\n",
    "    .withColumn(\"Number_of_veterans\",col(\"Number_of_veterans\").cast(\"int\"))\\\n",
    "    .withColumn(\"Foreign-born\",col(\"Foreign-born\").cast(\"int\"))\\\n",
    "    .withColumn(\"Count\",col(\"Count\").cast(\"int\"))\n",
    "\n",
    "#Drop null values\n",
    "demographics = demographics.na.drop(subset=['Male_population', 'Female_population', 'Number_of_veterans', 'Foreign-born', 'Average_household_size' ])\n",
    "\n",
    "demographics.head()\n",
    "demographics.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Temperature: double (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp = spark.read.load('../../data2/GlobalLandTemperaturesByCity.csv', format=INPUT_FORMAT, header=True).select('*')\n",
    "\n",
    "#Get avg temp by country\n",
    "temp = temp.groupby([\"Country\"]).agg({\"AverageTemperature\": \"avg\", \"Latitude\": \"first\", \"Longitude\": \"first\"})\\\n",
    "    .withColumnRenamed('avg(AverageTemperature)', 'Temperature')\\\n",
    "    .withColumnRenamed('first(Latitude)', 'Latitude')\\\n",
    "    .withColumnRenamed('first(Longitude)', 'Longitude')\n",
    "\n",
    "#make country caps\n",
    "temp = temp.withColumn(\"Country\",upper(col(\"Country\")))\n",
    "temp.head()\n",
    "temp.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: string (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#airport: city -> states\\\n",
    "airports = spark.read.load('airport-codes_csv.csv', format=INPUT_FORMAT, header=True).select('*')\n",
    "\n",
    "airports = airports.filter((airports.iso_country == 'US'))\n",
    "\n",
    "airports = airports.withColumn('iso_region', col('iso_region').substr(4,10)).withColumnRenamed('iso_region', 'state')\n",
    "airports.head()\n",
    "airports.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
